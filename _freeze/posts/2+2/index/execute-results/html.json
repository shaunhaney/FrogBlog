{
  "hash": "6c97a6579923adb3ee5d5382424aba66",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"2 + 2:  A Story of Why We Shouldn't Use AI For Everything\"\nauthor: \"Shaun Haney\"\ndate: \"2025-11-04\"\ncategories: ['AI', 'Machine Learning', 'Conservation', 'Ethics']\n---\n\n\n\nA few days ago, a former coworker on LinkedIn posed the question, \"Why are we using AI to generate code? Why don't let AI be the code?\" A lot of our use of agentic AI does just that, acting on instructions either directly from us in the form of prompts or from its internal data. But I would like to look at this question from a much simpler perspective to argue that there are some tasks handled so much more efficiently by traditional programs and that we should never be handing those tasks directly to AI. When I speak of AI in this post, I am referring to LLMs like GPT and more generally, transformers.\n\nI've decided to show my answer to this question by comparing the carbon footprint of calculating 2+2 in a traditional computer program to asking a model to do the job. I use a class called EmissionsTracker from a library called CodeCarbon to track how much compute power it takes to run each scenario. The indented code after the statement `with EmissionsTracker as tracker:` is what is being measured.\\\n2 + 2 is pretty atomic in terms of operations. It can usually be written as approximately 4 statements in assembler, a language just one level above machine language, where you load 2 into 2 registers, perform an add operation, and store it in another register. Even on primitive processors, it takes nanoseconds and barely registers any measurable energy expenditure.\\\nIf you're a computer scientist familiar with interpreters, you know that between assembler and Python, a lot more happens in between as the interface between Python and the machine has more layers of complexity and requires more energy.\\\nThe code segment below estimates the amount of energy required to run the statement `a=2+2` in Python.\n\n::: {#e65395d2 .cell execution_count=2}\n``` {.python .cell-code}\nfrom codecarbon import EmissionsTracker\n\na = 0\n# Create the tracker\nwith EmissionsTracker() as tracker:\n\n    # Simple statement that calculates 2+2 and stores it in a variable\n    a=2+2\nprint(f\"2+2={a}\")\n\ntraditional_emissions = tracker.final_emissions_data.emissions * 1_000_000_000\nprint(f\"Traditional Program → {traditional_emissions:.2f} µg CO₂e\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2+2=4\nTraditional Program → 4.48 µg CO₂e\n```\n:::\n:::\n\n\nAs you might expect, asking a model to add 2+2 together takes quite a bit more energy, but how much exactly? In this segment, we calculate the forward pass through GPT2. GPT2 is a predecessor to the current GPT5 model. GPT2 acts more like autocomplete and does not process instructions in the same way that GPT3 and later models do. The reason I've chosen GPT2 is because it is small enough to run locally, yet still big enough to show a few orders of magnitude difference in carbon emmissions when compared with running a traditional program. With each new version of GPT, the models have generally grown, scaling with a multitude more parameters and thus more calculations. While a traditional program ultimately calculates 2+2 as a single microprocessor operation, models act as statistical predictors, performing thousands to millions of addition operations during the forward pass to predict the answer to 2+2.\n\n::: {#b43b699e .cell execution_count=3}\n``` {.python .cell-code}\nfrom codecarbon import EmissionsTracker\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_name = \"gpt2\"                 \ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# ---- Track emissions for the inference ----\nprompt = \"2 + 2 =\"\nwith EmissionsTracker() as tracker:\n    # inputs need to first be tokenized\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    # Forward pass through GPT2\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=10,\n        do_sample=False,\n        temperature=0.0\n    )\n\n    new_text = tokenizer.decode(\n        outputs[0][inputs.input_ids.shape[1]:],\n        skip_special_tokens=True\n    )\nprint(\"2 + 2 =\", new_text.strip())\n\n# ---- Convert to micrograms CO₂e for readability ----\nmodel_emissions = tracker.final_emissions_data.emissions * 1_000_000_000\nprint(f\"Prompting a model → {model_emissions:.2f} µg CO₂e\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2 + 2 = 3.5 + 3.5 + 3.\nPrompting a model → 2823.09 µg CO₂e\n```\n:::\n:::\n\n\n::: { .sidebar-note } \nSo, you probably notice that GPT2 has given a nonsensical answer. As mentioned earlier, GPT2 is more of an autocompletion engine than its successors which actually have the ability to process instructions. I'd chosen the problem 2+2 for this article believing that 2+2=4 would be such a common pattern in any training data set, that I could keep focus on environmental impact, allowing numerous other articles to show much better examples of where AI can get wrong answers. Apparently, the training data for many early models – I tried many besides GPT2 – don't have enough examples of 2+2=4 to divulge that exact answer when prompted. Go figure! \n:::\nThis next cell shows that the difference in carbon emissions between calculating 2+2 in a traditional program and prompting GPT2 for the same problem is orders of magnitude difference in carbon emissions. Consider when looking at this calculation that GPT2 has \\~1.5 billion parameters, but that GPT5 may have \\~1.7 trillion parameters. This estimate puts GPT5 at roughly 1000 times the size of GPT2, which means it probably uses 100-1000 times the amount of energy for the same prompt. This why we have [air pollution situations that can be traced directly to power plants fueling data centers for AI.](https://www.politico.com/news/2025/05/06/elon-musk-xai-memphis-gas-turbines-air-pollution-permits-00317582)\n\n::: {#bbba588e .cell execution_count=4}\n``` {.python .cell-code}\nprint(f\"Using GPT2 to answer 2+2 generates {model_emissions/traditional_emissions:.0f} times as much CO₂ as calculating 2+2 in Python.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing GPT2 to answer 2+2 generates 630 times as much CO₂ as calculating 2+2 in Python.\n```\n:::\n:::\n\n\nSo, my argument is not to stop using AI. AI is already widely used and adopted. However, considering how widely AI is used and what its capabilities are, I think it is a smart decision to have AI use traditional programs as tooling, whether AI itself generates and uses those programs or humans provide those programs. Having AI delegate its weaker points to tooling, rather than trying to use AI as the only tool will help mitigate AI's contribution to climate change.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}