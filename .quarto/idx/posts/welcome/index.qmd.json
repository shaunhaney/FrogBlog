{"title":"2 + 2:  A Story of Why We Shouldn't Use AI For Everything","markdown":{"yaml":{"title":"2 + 2:  A Story of Why We Shouldn't Use AI For Everything","author":"Shaun Haney","date":"2025-10-27","categories":["AI","Machine Learning","Conservation","Ethics"]},"headingText":"Hush anything not explicitly printed","containsRefs":false,"markdown":"\n\n```{python}\n#| echo: false \nimport logging\nlogging.disable(logging.CRITICAL)\n```\n\nA few days ago, a former coworker on LinkedIn posed the question, \"Why are we using AI to generate code? Why don't let AI be the code?\" A lot of our use of agentic AI does just that, acting on instructions either directly from us in the form of prompts or from its internal data. But I would like to look at this question from a much simpler perspective to argue that there are some tasks handled so much more efficiently by traditional programs and that we should never be handing those tasks directly to AI. When I speak of AI in this post, I am referring to LLMs like GPT and more generally, transformers.\n\nI've decided to show my answer to this question by comparing the carbon footprint of calculating 2+2 in a traditional computer program to asking a model to do the job. I use a class called EmissionsTracker from a library called CodeCarbon to track how much compute power it takes to run each scenario. The indented code after the statement `with EmissionsTracker as tracker:` is what is being measured.\\\n2 + 2 is pretty atomic in terms of operations. It can usually be written as approximately 4 statements in assembler, a language just one level above machine language, where you load 2 into 2 registers, perform an add operation, and store it in another register. Even on primitive processors, it takes nanoseconds and barely registers any measurable energy expenditure.\\\nIf you're a computer scientist familiar with interpreters, you know that between assembler and Python, a lot more happens in between as the interface between Python and the machine has more layers of complexity and requires more energy.\\\nThe code segment below estimates the amount of energy required to run the statement `a=2+2` in Python.\n\n```{python}\nfrom codecarbon import EmissionsTracker\n\na = 0\n# Create the tracker\nwith EmissionsTracker() as tracker:\n\n    # Simple statement that calculates 2+2 and stores it in a variable\n    a=2+2\nprint(f\"2+2={a}\")\n\ntraditional_emissions = tracker.final_emissions_data.emissions * 1_000_000_000\nprint(f\"Traditional Program → {traditional_emissions:.2f} µg CO₂e\")\n```\n\nAs you might expect, asking a model to add 2+2 together takes quite a bit more energy, but how much exactly? In this segment, we calculate the forward pass through GPT2. GPT2 is a predecessor to the current GPT5 model. GPT2 acts more like autocomplete and does not process instructions in the same way that GPT3 and later models do. The reason I've chosen this model here is because it is small enough to run locally, yet still big enough to demonstrate my point. With each new version of GPT, the models have generally grown, scaling with a multitude more parameters and thus more calculations. While a traditional program ultimately calculates 2+2 as a single microprocessor operation, models act as statistical predictors, performing thousands to millions of addition operations during the forward pass to predict the answer to 2+2.\n\n```{python}\nfrom codecarbon import EmissionsTracker\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_name = \"gpt2\"                 \ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# ---- Track emissions for the inference ----\nprompt = \"2 + 2 =\"\nwith EmissionsTracker() as tracker:\n    # inputs need to first be tokenized\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    # Forward pass through GPT2\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=10,\n        do_sample=False,\n        temperature=0.0\n    )\n\n    new_text = tokenizer.decode(\n        outputs[0][inputs.input_ids.shape[1]:],\n        skip_special_tokens=True\n    )\nprint(\"2 + 2 =\", new_text.strip())\n\n# ---- Convert to micrograms CO₂e for readability ----\nmodel_emissions = tracker.final_emissions_data.emissions * 1_000_000_000\nprint(f\"Prompting a model → {model_emissions:.2f} µg CO₂e\")\n\n```\n\nSo, what I want you to notice here is how much significantly higher the estimated carbon emissions are for the model run. I was actually hoping GPT2 would produce the answer 4 as well, since \"2 + 2 = 4\" is or should be a fairly common piece of online text. GPT2 is predicting the next token differently, and it should be noted, is not calculating 2+2 at all. That's not what LLMs do unless they delegate the task out to tooling. On the other hand, if you were to ask GPT5 what 2+2 is, it would predict 4 for you without an issue. But the question I am asking here is, \"But at what cost?\" Possiby enough energy to make a light bulb flash. That difference in energy expenditure points to AI being a significant contributor to climate change and global warming when the electricity is generated using coal and fossil fuels.\\\nThe next code block prints how much more energy AI uses for the 2+2 operation over traditional code.\n\n```{python}\nprint(f\"Using GPT2 to answer 2+2 generates {model_emissions/traditional_emissions:.0f} times as much CO₂ as calculating 2+2 in Python.\")\n```\n\nSo, my argument is not to stop using AI. AI is already widely used and adopted. However, considering how widely AI is used and what its capabilities are, I think it is a smart decision to have AI use traditional programs as tooling, whether AI generates those programs and uses them directly or humans help develop those programs. This approach to tooling, rather than trying to use AI as the only tool will help mitigate AI's contribution to climate change.","srcMarkdownNoYaml":"\n\n```{python}\n#| echo: false \n# Hush anything not explicitly printed\nimport logging\nlogging.disable(logging.CRITICAL)\n```\n\nA few days ago, a former coworker on LinkedIn posed the question, \"Why are we using AI to generate code? Why don't let AI be the code?\" A lot of our use of agentic AI does just that, acting on instructions either directly from us in the form of prompts or from its internal data. But I would like to look at this question from a much simpler perspective to argue that there are some tasks handled so much more efficiently by traditional programs and that we should never be handing those tasks directly to AI. When I speak of AI in this post, I am referring to LLMs like GPT and more generally, transformers.\n\nI've decided to show my answer to this question by comparing the carbon footprint of calculating 2+2 in a traditional computer program to asking a model to do the job. I use a class called EmissionsTracker from a library called CodeCarbon to track how much compute power it takes to run each scenario. The indented code after the statement `with EmissionsTracker as tracker:` is what is being measured.\\\n2 + 2 is pretty atomic in terms of operations. It can usually be written as approximately 4 statements in assembler, a language just one level above machine language, where you load 2 into 2 registers, perform an add operation, and store it in another register. Even on primitive processors, it takes nanoseconds and barely registers any measurable energy expenditure.\\\nIf you're a computer scientist familiar with interpreters, you know that between assembler and Python, a lot more happens in between as the interface between Python and the machine has more layers of complexity and requires more energy.\\\nThe code segment below estimates the amount of energy required to run the statement `a=2+2` in Python.\n\n```{python}\nfrom codecarbon import EmissionsTracker\n\na = 0\n# Create the tracker\nwith EmissionsTracker() as tracker:\n\n    # Simple statement that calculates 2+2 and stores it in a variable\n    a=2+2\nprint(f\"2+2={a}\")\n\ntraditional_emissions = tracker.final_emissions_data.emissions * 1_000_000_000\nprint(f\"Traditional Program → {traditional_emissions:.2f} µg CO₂e\")\n```\n\nAs you might expect, asking a model to add 2+2 together takes quite a bit more energy, but how much exactly? In this segment, we calculate the forward pass through GPT2. GPT2 is a predecessor to the current GPT5 model. GPT2 acts more like autocomplete and does not process instructions in the same way that GPT3 and later models do. The reason I've chosen this model here is because it is small enough to run locally, yet still big enough to demonstrate my point. With each new version of GPT, the models have generally grown, scaling with a multitude more parameters and thus more calculations. While a traditional program ultimately calculates 2+2 as a single microprocessor operation, models act as statistical predictors, performing thousands to millions of addition operations during the forward pass to predict the answer to 2+2.\n\n```{python}\nfrom codecarbon import EmissionsTracker\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_name = \"gpt2\"                 \ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# ---- Track emissions for the inference ----\nprompt = \"2 + 2 =\"\nwith EmissionsTracker() as tracker:\n    # inputs need to first be tokenized\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    # Forward pass through GPT2\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=10,\n        do_sample=False,\n        temperature=0.0\n    )\n\n    new_text = tokenizer.decode(\n        outputs[0][inputs.input_ids.shape[1]:],\n        skip_special_tokens=True\n    )\nprint(\"2 + 2 =\", new_text.strip())\n\n# ---- Convert to micrograms CO₂e for readability ----\nmodel_emissions = tracker.final_emissions_data.emissions * 1_000_000_000\nprint(f\"Prompting a model → {model_emissions:.2f} µg CO₂e\")\n\n```\n\nSo, what I want you to notice here is how much significantly higher the estimated carbon emissions are for the model run. I was actually hoping GPT2 would produce the answer 4 as well, since \"2 + 2 = 4\" is or should be a fairly common piece of online text. GPT2 is predicting the next token differently, and it should be noted, is not calculating 2+2 at all. That's not what LLMs do unless they delegate the task out to tooling. On the other hand, if you were to ask GPT5 what 2+2 is, it would predict 4 for you without an issue. But the question I am asking here is, \"But at what cost?\" Possiby enough energy to make a light bulb flash. That difference in energy expenditure points to AI being a significant contributor to climate change and global warming when the electricity is generated using coal and fossil fuels.\\\nThe next code block prints how much more energy AI uses for the 2+2 operation over traditional code.\n\n```{python}\nprint(f\"Using GPT2 to answer 2+2 generates {model_emissions/traditional_emissions:.0f} times as much CO₂ as calculating 2+2 in Python.\")\n```\n\nSo, my argument is not to stop using AI. AI is already widely used and adopted. However, considering how widely AI is used and what its capabilities are, I think it is a smart decision to have AI use traditional programs as tooling, whether AI generates those programs and uses them directly or humans help develop those programs. This approach to tooling, rather than trying to use AI as the only tool will help mitigate AI's contribution to climate change."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":["cosmo","brand"],"title-block-banner":true,"title":"2 + 2:  A Story of Why We Shouldn't Use AI For Everything","author":"Shaun Haney","date":"2025-10-27","categories":["AI","Machine Learning","Conservation","Ethics"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}